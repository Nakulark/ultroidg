# -*- coding: utf-8 -*-
"""Nakul_foot_Ball_PROJECT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uHrlsDyW3Q0zDC7XkaD9ao7LxJly_Dz-
"""

# Import libraies
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# load our dataset
df=pd.read_csv('/content/Data set__1682565729-62fcaa4ea2bc0b6439d2306a (1).csv')

# check dataset load
df.head()

# info show us nature and how our data is presnet in dataframe as data type and how how missing values are there
df.info()

# describe show us statistics analysis
df.describe()

# it's returns the number of missing values in the dataset
df.isnull().sum()

# it's return count of unique values
df['region'].value_counts()

# fill missing values by mean
df['region']=df['region'].fillna(2.0)

df.columns

# remove unwanted column or feature
df1=df.drop(columns=['name'])
df1.shape

# merging this two columns because both are same
df1['total_signing']=df1['new_foreign']+df1['new_signing']

df1=df1.drop(['new_signing','new_foreign'],axis=1)

# Market value Distribution
plt.figure(figsize=(8,5))
sns.histplot(df1['market_value'],kde=True,bins=30)
plt.title('Market Value Distribution')
plt.xlabel('Market Value')
plt.ylabel('Count')
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns

# below is correlation heatmap for analyzing suitable features for us
plt.figure(figsize=(8,6))
sns.heatmap(df1.corr(numeric_only=True),annot=True,cmap="coolwarm")
plt.title("Correlation Heatmap")
plt.show()

# below show outlier of best features which high correlation with market value
plt.figure(figsize=(8,5))
sns.boxplot(x=df1['market_value'])
plt.title("Market Value Outliers")
plt.show()

plt.figure(figsize=(8,5))
sns.boxplot(x=df1['fpl_points'])
plt.title('Fpi ponts Outliers')
plt.show()

plt.figure(figsize=(8,5))
sns.boxplot(x=df1['page_views'])
plt.title('Page Views Outliers')
plt.show()

plt.figure(figsize=(8,5))
sns.boxplot(x=df1['fpl_value'])
plt.title('Fpl value Outliers')
plt.show()

plt.figure(figsize=(8,5))
sns.boxplot(x=df1['fpl_sel'])
plt.title('Fpl Sel Outliers')
plt.show()

# compare market value by region
plt.figure(figsize=(8,5))
sns.boxplot(x="region",y="market_value",data=df1)
plt.show()

# in this step remove outliers from our data for better result

cols = ["market_value", "fpl_points", "page_views", "fpl_value"]

foot_ball=df1.copy()

print("Orignal shape:",foot_ball.shape)

for col in cols:
  if col in foot_ball.columns:
    Q1=foot_ball[col].quantile(0.25) #25th percentile
    Q3=foot_ball[col].quantile(0.75) #75th percentile
    IQR=Q3-Q1

    # Lower and Upper limit
    lower=Q1-1.5*IQR
    upper=Q3+1.5*IQR

    # filter data
    before=foot_ball.shape[0]
    foot_ball=foot_ball[(foot_ball[col]>=lower)&(foot_ball[col]<=upper)]
    after=foot_ball.shape[0]

    print(f"{col}: Removed {before-after} rows")

print("Final shape after removing outliers:",foot_ball.shape)

# Encoding for convert catagorical columns into numerical
# below is Lebel Encoding

from sklearn.preprocessing import LabelEncoder

cat_col=[col for col in foot_ball.columns if foot_ball[col].dtype=='object']
label_encoder=LabelEncoder()
for col in cat_col:foot_ball[col + '_encoded']=label_encoder.fit_transform(foot_ball[col])#this loop iterates through each column name present in the cat_col list
# for each ctegorical column, a new column is created by using the append '_encoded' to the original column name

display(foot_ball.head()) #display the first few rows with the new encoded columns

foot_ball.info()

# delete catogorical features beacause we do encoding above
foot_ball=foot_ball.drop(['club','position','nationality'],axis=1)

foot_ball.info()

# Perform Normalization that transforms numerical features to a common scale between -1 to 1
# First we use Min-Max scaling (subtracting the minimum value / difference between maximum and minimum values) and ordinal encoded data
from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler() #initializes the instance of MinMaxScaler

# Select only numerical columns for scaling
num_col = df1.select_dtypes(include=np.number).columns

scaled_data = scaler.fit_transform(df1[num_col]) #fit method calculates the Min Max values for each column (feature) in the df1 & stored within the scaler object
scaled_df1 = pd.DataFrame(scaled_data, columns = num_col) #the scaled data is NumPy array and this line converts it into a pandas DataFrame, keeping the original column names

display(scaled_df1.head())

# Perform Standardization on the Ordinal Coded data transforming the feature to have a mean of 0
# and standard deviation (variance) of 1

from sklearn.preprocessing import StandardScaler

# initiate scaler
scaler = StandardScaler()

# Select the numerical column for scaling
num_col = df1.select_dtypes(include=np.number).columns

# fit the data and transform
scaled_data = scaler.fit_transform(df1[num_col])

# the scaled data is NumPy array and this line converts it into a pandas DataFrame, keeping the original column names
scaled_dfdata = pd.DataFrame(scaled_data, columns = num_col)

display(scaled_dfdata.head())

import matplotlib.pyplot as plt
import seaborn as sns

# below is correlation heatmap for analyzing suitable features for us
plt.figure(figsize=(8,6))
sns.heatmap(scaled_df1.corr(numeric_only=True),annot=True,cmap="coolwarm")
plt.title("Correlation Heatmap")
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns

# below is correlation heatmap for analyzing suitable features for us
plt.figure(figsize=(8,6))
sns.heatmap(scaled_dfdata.corr(numeric_only=True),annot=True,cmap="coolwarm")
plt.title("Correlation Heatmap")
plt.show()

# we will first train and find results on Univariate Linear Regression using the Mean Square Error cost function.
# After calculating the cost function, we need to mitigate the error and it is done through Gradient Descent

# import the libraries
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

# Define the feature (independent variable) and target (dependent variable)
X = scaled_df1.drop(['market_value'],axis=1)  # we use double brackets, so to keep the output as a DataFrame
y = scaled_df1['market_value']

# split the data into training and testing sets (80% train and 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)


# create a linear regression model
model = LinearRegression()

# model is trained, now we can use it to make predictions on test data
# predict Market Value

model.fit(X_train,y_train)
y_pred = model.predict(X_test)

mse = mean_squared_error(y_test, y_pred)
print(f"Mean Squared Error: {mse}")

r2 = r2_score(y_test, y_pred)
print(f"R2 Score: {r2}")

from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.linear_model import Ridge
from sklearn.metrics import mean_squared_error, r2_score, make_scorer

# Initialize Feature and target
X = scaled_dfdata.drop(['market_value'], axis=1)
y = scaled_dfdata['market_value']

# Train test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define scoring metrics
scoring_metrics = {
    'MSE': make_scorer(mean_squared_error, greater_is_better=False),
    'R2': make_scorer(r2_score)
}

# Define the parameter grid for alpha
param_grid = {'alpha': [0.1, 1.0, 10.0, 100.0]}

# Grid search with Ridge regression only
grid_search = GridSearchCV(Ridge(), param_grid, cv=5, scoring=scoring_metrics, refit='R2')
grid_search.fit(X_train, y_train)

print(f"Optimal alpha: {grid_search.best_params_['alpha']}")

# Get the best model
best_model = grid_search.best_estimator_

# Predict on the test set
y_pred = best_model.predict(X_test)

# Evaluate the model
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"Mean Squared Error: {mse}")
print(f"R2 Score: {r2}")

from sklearn.model_selection import GridSearchCV, train_test_split
from sklearn.linear_model import Lasso
from sklearn.metrics import mean_squared_error, r2_score
import matplotlib.pyplot as plt

# Features & Target
X = scaled_dfdata.drop(['market_value'], axis=1)
y = scaled_df1['market_value']

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)


lasso = Lasso()
param_grid = {
    "alpha": [0.0001, 0.001, 0.01, 0.1, 1, 10],  # Regularization strength
    "max_iter": [1000, 5000, 10000]              # Iteration limit
}

grid_lasso = GridSearchCV(
    estimator=lasso,
    param_grid=param_grid,
    cv=5,
    scoring="r2",
    refit=True
)

# Fit
grid_lasso.fit(X_train, y_train)

# Best params
print("✅ Best Params for Lasso:", grid_lasso.best_params_)

# Best model
best_lasso = grid_lasso.best_estimator_

# Predictions
y_pred = best_lasso.predict(X_test)

# Metrics
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
print(f"Lasso Regression (Tuned): MSE={mse:.2f}, R²={r2:.2f}")

# -----------------------------
# Plot
# -----------------------------
plt.scatter(y_test, y_pred, color="green", alpha=0.6, label="Predicted vs Actual")
plt.xlabel("Actual Market Value")
plt.ylabel("Predicted Market Value")
plt.title("Lasso Regression with GridSearchCV")
plt.legend()
plt.show()

# next model knn

from sklearn.neighbors import KNeighborsRegressor

# Features aur target
X = scaled_dfdata.drop(['market_value'],axis=1)
y = scaled_dfdata['market_value']

# Scaling


# Train-test split (ab scaled X use karenge)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)


grid_params = { 'n_neighbors' : [5,7,9,11,13,15],
               'weights' : ['uniform','distance'],
               'metric' : ['minkowski','euclidean','manhattan']}

grid_search = GridSearchCV(KNeighborsRegressor(), grid_params, verbose = 1, cv=5, n_jobs = -1)

# Train + Evaluate

grid_search.fit(X_train, y_train)
print("Best Params:", grid_search.best_params_)

best_pipeline=grid_search.best_estimator_
preds = best_pipeline.predict(X_test)

mse = mean_squared_error(y_test, preds)

r2 = r2_score(y_test, preds)

print(f"Mean SQUARE ERROR {mse}")
print(f"R2 SCORE {r2}")
# print(f"{name}: MSE = {mse:.2f}, R2 Score = {r2:.2f}")

from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import GridSearchCV, train_test_split
from sklearn.metrics import mean_squared_error, r2_score

# Features aur target
X = scaled_dfdata.drop('market_value', axis=1)
y = scaled_dfdata['market_value']

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# RandomForestRegressor
rf = RandomForestRegressor(random_state=42)

# Hyperparameter grid
param_grid = {
    'n_estimators': [100, 200],
    'max_depth': [10, 20, None],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

# GridSearchCV
grid_search = GridSearchCV(
    estimator=rf,
    param_grid=param_grid,
    cv=3,
    n_jobs=-1,
    verbose=2,
    scoring='r2'
)

# Fit model
grid_search.fit(X_train, y_train)

# Best params
print("Best Params:", grid_search.best_params_)

# Best estimator
best_rf = grid_search.best_estimator_

# Predictions
y_pred = best_rf.predict(X_test)

# Evaluation
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"Mean Squared Error: {mse}")
print(f"R2 Score: {r2}")

from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.pipeline import Pipeline
from sklearn.metrics import mean_squared_error, r2_score

# Features & Target
x = scaled_df1.drop(['market_value'], axis=1)
y = scaled_df1['market_value']

# Train-test split
x_train, x_test, y_train, y_test = train_test_split(
    x, y, test_size=0.2, random_state=42
)
print(f"Training Set Size: {x_train.shape[0]}")
print(f"Testing Set Size: {x_test.shape[0]}")


dtr=DecisionTreeRegressor(random_state=42)

# Hyperparameter grid (no n_estimators here!)
param_grid = {
    'max_depth': [4, 6, 10, None],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

# GridSearchCV
grid_search = GridSearchCV(
    dtr, param_grid,
    cv=5, scoring='neg_mean_squared_error', n_jobs=-1
)
grid_search.fit(x_train, y_train)

# Best params
print("Best Params:", grid_search.best_params_)

# Best estimator
best_dtr = grid_search.best_estimator_

# Predictions
y_pred = best_dtr.predict(x_test)

# Evaluation
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
print(f"Mean Squared Error: {mse}")
print(f"R2 Score: {r2}")


import matplotlib.pyplot as plt

# Scatter plot: Actual vs Predicted
plt.figure(figsize=(8,6))
plt.scatter(y_test, y_pred, alpha=0.7, color='blue', edgecolor="k")
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')

plt.xlabel("Actual Market Value")
plt.ylabel("Predicted Market Value")
plt.title("Decision Tree Regression: Actual vs Predicted")
plt.show()

